{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e6eaa1",
   "metadata": {},
   "source": [
    "# Introduction to the Project\n",
    "\n",
    "Hey there, this notebook shall go through the a very famous, yet overlooked problem - Credit Card Fraud Detection. This project is special, in the sense that this project is highly imbalanced. To solve this problem we'll use techniques like **SMOTE**, and we'll surely learn something new.\n",
    "\n",
    "Especially, in these volatile cases, where there is a very less amount of frauds, you need to make a classification machine learning model that'd predict if a transaction is fraud or not - and without some pre-processing, our model would always predict as **NOT** fraud, even when it is!\n",
    "\n",
    "So this project mainly revovles around the idea of imbalance and the techniques used to balance our data before modelling\n",
    "\n",
    "# Data\n",
    "The dataset contains transactions made by credit cards in September 2013 by European cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. \n",
    "\n",
    "# Evaluation\n",
    "Given the class imbalance ratio, we'll measure the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification\n",
    "\n",
    "As this is just a dataset (And not a Kaggle competition), we'll take 0.2% of our data as our test set, and then we'll begin modelling.\n",
    "\n",
    "The Dataset/Kaggle Link: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11811d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Pre-Processing Libs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Modelling Libs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Validating/Testing libs\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "print(\"Libraries Imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba9e78",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "* We'll have to unzip a folder (I had to zip it becuase the file was too big, and GitHub doesn't allow to push 100+ MB) - This can be done with Pandas...\n",
    "\n",
    "* Look at our data\n",
    "\n",
    "* Some Information about our data\n",
    "\n",
    "* A bit of description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efbc567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset using the compression zip\n",
    "credit_df = pd.read_csv('creditcard.csv.zip',compression='zip')\n",
    " \n",
    "# Display dataset\n",
    "credit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Information\n",
    "credit_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5501cc",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "Well this is not bad. No categorical/string columns are included. All columns except for `Time` and `Amount` are transformed by **PCA** and have been **Scaled**.\n",
    "\n",
    "So we'll have to **PCA** transform and **Scale** these Columns. But before we do anything like this, we'll do some Exploratory Data Analysis (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d48700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Some Description\n",
    "credit_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea70f4fe",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "There isn't much to say. The features `V1-V28` are anonymous and we have no information whatsoever. This is so because these columns have some confidential information that cannot be disclosed to the general public. But these columns are well-processed (PCA Transformation, Dimensiality Reduction and Scaling), so no worries.\n",
    "\n",
    "We just need to deal with the **Time** and the **Amount** column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0d5b85",
   "metadata": {},
   "source": [
    "# Some EDA\n",
    "* Look at the distribution of the `Time`, `Amount` and `Class` column\n",
    "* Experience the horrible imbalancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ddd787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Time\n",
    "px.histogram(x=credit_df[\"Time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58218377",
   "metadata": {},
   "source": [
    "**Distribution**\n",
    "\n",
    "I see no pattern except that a wave like pattern (Starting low, moving up, going low and then moving up again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e94a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Time\n",
    "px.histogram(x=credit_df[\"Amount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c2b2b",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "Most values are around 0-100, and there are rare cases with more than 5k. But we can't consider them as outliers as it is very much possible to transfer over 5k to anyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e33e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frauds and Non-Frauds\n",
    "plt.figure(figsize=(8, 5), dpi=120)\n",
    "credit_df.Class.value_counts().plot(kind=\"pie\", explode=[0, 0.1], shadow=True, startangle=140, autopct='%1.1f%%')\n",
    "plt.legend(labels=['Normal','Fraud'])\n",
    "plt.title('\"Fraud\" Distribution')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f9cde4",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "So as you can see, there is only 0.2% fraud (570 Samples from 284,807 entries), which is a severe imbalance. If we train our model just like this, there is no chance we'll ever predict a **FRAUD** case. So we'll have to deal with this and this project is mainly about this topic - Dealing with Imbalanced Classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a474717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation of Non-Frauds and Frauds with Transaction Time\n",
    "values = credit_df[\"Class\"].value_counts().index\n",
    "figure, (non_fraud, fraud) = plt.subplots(2,1, sharex=True, figsize=(15, 10))\n",
    "\n",
    "non_fraud.hist((credit_df[\"Time\"]/60/60)[credit_df[\"Class\"] == 0], bins=50, color=\"lightgreen\")\n",
    "non_fraud.set_title(\"Class = NON-FRAUD\")\n",
    "\n",
    "fraud.hist((credit_df[\"Time\"]/60/60)[credit_df[\"Class\"] ==1 ], bins=50, color=\"salmon\")\n",
    "fraud.set_title(\"Class = FRAUD\")\n",
    "\n",
    "plt.xticks(np.arange(0,54,6))\n",
    "plt.xlim([0,48])\n",
    "plt.xlabel(\"Time after first transaction (HOURS)\")\n",
    "plt.ylabel('Number of Transactions')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e35fd",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "As you can see, the number of transactions for genuine users take a hit during late night and early morning hours. It also makes sense since most people sleep during this. On the contrary, for fraudulent transactions, the number sees sharp spikes during late hours, and during the daytime, the count is significantly less.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941eec5a",
   "metadata": {},
   "source": [
    "# Cleaning Data\n",
    "* Missing Data?\n",
    "* Duplicating Data?\n",
    "* Outliers?\n",
    "\n",
    "Let's clean the data a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a copy and do all the wrangling stuff on there so we have our orignal dataset preserved\n",
    "credit_df_copy = credit_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a1d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df_copy.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f72619",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "No **NULL** values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaeeef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicating Data (Number of Columns)\n",
    "print(f\"Non-Frauds: {credit_df_copy[credit_df_copy.Class == 0].duplicated().sum()}\")\n",
    "print(f\"Frauds: {credit_df_copy[credit_df_copy.Class == 1].duplicated().sum()}\")\n",
    "print(\"*\" * 100)\n",
    "\n",
    "# Drop\n",
    "credit_df_copy.drop_duplicates(inplace=True)\n",
    "print(\"Dropped Succesfully\")\n",
    "print(\"*\" * 100)\n",
    "\n",
    "# Check\n",
    "print(f\"Non-Frauds: {credit_df_copy[credit_df_copy.Class == 0].duplicated().sum()}\")\n",
    "print(f\"Frauds: {credit_df_copy[credit_df_copy.Class == 1].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03031990",
   "metadata": {},
   "source": [
    "Regarding outliers, we'll not deal with them. Becuase all is possible. The amount could be easily over 5k and the time could be more becuase of internet or any technical issue. So I don't think there will be any outliers as this dataset seems to be constructed by something automated, and not manual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbad154",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "* PCA Transforming the `Time` & `Amount` columns\n",
    "* Using the `RobustScaler()` to scale the `Time` & `Amount` columns\n",
    "* Using `SMOTE` technique to solve the imbalancy\n",
    "\n",
    "\n",
    "**Some Recourses**\n",
    "\n",
    "1. To learn more about PCA transformations, you can read this: https://builtin.com/data-science/step-step-explanation-principal-component-analysis\n",
    "\n",
    "2. To learn about Scaling, read this: https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/\n",
    "\n",
    "3. To read on RobustScaler, have a look here: https://www.geeksforgeeks.org/standardscaler-minmaxscaler-and-robustscaler-techniques-ml/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5744e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA transformations\n",
    "pca = PCA(n_components = 2)\n",
    "columns = credit_df_copy[[\"Time\", \"Amount\"]]\n",
    "pca.fit(columns)\n",
    "credit_df_copy[[\"Time\", \"Amount\"]] = pca.transform(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b74101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling with the Robust Scaler\n",
    "transformer = RobustScaler().fit(columns)\n",
    "credit_df_copy[[\"Time\", \"Amount\"]] = transformer.transform(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b8909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SMOTE to balance the data\n",
    "X = credit_df_copy.drop('Class', axis = 1)\n",
    "y = credit_df_copy['Class']\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X, y = smote.fit_resample(X, y)\n",
    "\n",
    "# Plot the results\n",
    "fig = px.pie(values=y.value_counts(), \n",
    "             width=800, height=400, \n",
    "             title=\"Data Balance\",\n",
    "             color_discrete_sequence=[\"skyblue\",\"black\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2283434",
   "metadata": {},
   "source": [
    "Beautiful, isn't it? We did some PCA transformations, did some Scaling using the RobustScaler method and also balanced our dataset!\n",
    "\n",
    "Now....we move to the fun part - the modelling part. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f37cef1",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "* Split the Data into Training & Testing sets\n",
    "* Try out different models like\n",
    "    1. Logistic Regression\n",
    "    2. Naive Bayes (GaussianNB)\n",
    "    3. Random Forest Classifier\n",
    "    4. K-Neighbors Classifier\n",
    "    5. XGBoost Classifier\n",
    "\n",
    "* Then we'll do some testing on our test set, push our project to [Github](https://github.com/muhammadanas0716/Machine-Learning-Projects-101) and [Kaggle](https://www.kaggle.com/muhammadanas0716)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f19a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(f\"\"\"Data Splitted. Here are the Stats:\n",
    "\n",
    "Rows in X_train: {X_train.shape[0]}\n",
    "Rows in y_train: {y_train.shape[0]}\n",
    "\n",
    "Rows in X_test: {X_test.shape[0]}\n",
    "Rows in y_test: {y_test.shape[0]} \n",
    "\n",
    "Columns in X_train & X_test are 3\n",
    "Columns in y_train & y_test is only 1 - the TARGET column (i.e Class)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc671bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train , y_train)\n",
    "classifier_score = classifier.score(X_test , y_test).round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69176a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dt =DecisionTreeClassifier(max_features=8 , max_depth=6)\n",
    "dt.fit(X_train , y_train)\n",
    "dt_score = dt.score(X_test , y_test).round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "Rclf = RandomForestClassifier(max_features=8 , max_depth=6)\n",
    "Rclf.fit(X_train, y_train)\n",
    "Rclf_score = Rclf.score(X_test, y_test).round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe560894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(C = 100, max_iter=1000)\n",
    "lr.fit(X_train , y_train)\n",
    "lr_score = lr.score(X_test , y_test).round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06315b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_score = knn.score(X_test, y_test).round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d30d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train , y_train)\n",
    "xgb_score = xgb.score(X_test, y_test).round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eafbd6",
   "metadata": {},
   "source": [
    "# Testing Metrics\n",
    "\n",
    "We'll use the following metrics:\n",
    "* Accuracy\n",
    "* F-1 Score\n",
    "* Precision Score\n",
    "* Recall Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a5bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison = {}\n",
    "names = [\"Decision Tree\", \"Naive\", \"Random Forest\", \"KNN\", \"Logistic Regression\", \"XGboost\"]\n",
    "models = [classifier, dt, Rclf, lr, knn, xgb]\n",
    "results = {}\n",
    "\n",
    "# Make Predictions\n",
    "for model in models:\n",
    "    results[str(model).split(\"(\")[0]] = [model.predict(X_test)]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df2590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the scores of the metrics\n",
    "for model, preds in results.items():\n",
    "    model_comparison[model] = [\n",
    "                            round(accuracy_score(y_test, pd.DataFrame(preds).T), 2),\n",
    "                            round(f1_score(y_test, pd.DataFrame(preds).T,average='weighted'), 2),\n",
    "                            round(precision_score(y_test, pd.DataFrame(preds).T), 2),\n",
    "                            round(recall_score(y_test, pd.DataFrame(preds).T), 2),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(model_comparison, index=[\"Accuracy\", \"F-1 Score\", \"Precision Score\", \"Recall Score\"])\n",
    "results_df.style.format(\"{:.2%}\").background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69d4940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Scores\n",
    "classifier_cr = cross_val_score(classifier, credit_df_copy.drop(\"Class\", axis=1), credit_df_copy[\"Class\"], cv=5).mean()\n",
    "dt_cr = cross_val_score(dt, credit_df_copy.drop(\"Class\", axis=1), credit_df_copy[\"Class\"], cv=5).mean()\n",
    "Rclf_cr = cross_val_score(Rclf, credit_df_copy.drop(\"Class\", axis=1), credit_df_copy[\"Class\"], cv=5).mean()\n",
    "lr_cr = cross_val_score(lr, cre`dit_df_copy.drop(\"Class\", axis=1), credit_df_copy[\"Class\"], cv=5).mean()\n",
    "knn_cr = cross_val_score(knn, credit_df_copy.drop(\"Class\", axis=1), credit_df_copy[\"Class\"], cv=5).mean()\n",
    "xgb_cr = cross_val_score(xgb, credit_df_copy.drop(\"Class\", axis=1), credit_df_copy[\"Class\"], cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fa4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Scores in a Plot\n",
    "cross_validated_scores = [classifier_cr, dt_cr, Rclf_cr, lr_cr, knn_cr, xgb_cr]\n",
    "cross_validated_scores = pd.DataFrame(cross_validated_scores, index=[\"GaussianNB\", \n",
    "                                            \"DecisionTreeClassifier\", \n",
    "                                            \"RandomForestClassifier\",\n",
    "                                            \"LogisticRegression\",\n",
    "                                            \"KNeighborsClassifier\", \n",
    "                                            \"XGBClassifier\"])\n",
    "cross_validated_scores.rename(columns={0 : \"Score\"}, inplace=True)\n",
    "cross_validated_scores.plot(kind=\"bar\", figsize=(10, 5), color=[\"salmon\"])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c633cafd",
   "metadata": {},
   "source": [
    "# End Notes\n",
    "And there...our model works amazingly fine! The scores are dope. KNN and XGBClassifier did really well, giving us a 100% accuracy, but I would be skeptical of this. But as we did cross-validation, maybe this is not bad after all!\n",
    "\n",
    "KNN seems to be the BEST algorithm for this problem!\n",
    "For now my freinds that's it! And be sure to give this repo a star and follow me [@MuhammadAnas707](https://twitter.com/MuhammadAnas707)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
